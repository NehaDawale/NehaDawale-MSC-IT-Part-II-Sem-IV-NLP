{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practical 11NLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOzq7x2K0AhQT4EXwWj11AP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Practical 11**"],"metadata":{"id":"UAv_hBEaa_G7"}},{"cell_type":"markdown","source":["**A) Aim-:Multiword Expressions in NLP**"],"metadata":{"id":"c0Pu7A46xG0P"}},{"cell_type":"markdown","source":["**Multi-word Expressions** (MWEs) are word combinations with linguistic properties that cannot be predicted from the properties of the individual words or the way they have been combined. MWEs occur frequently and are usually highly domain-dependent. A proper treatment of MWEs is essential for the success of NLP-systems. \n","\n","eneric NLP-systems usually perform less well on texts from specific domains. One of the reasons for this is clear: each domain uses its own vocabulary, and it uses generally occurring words with a highly specific meaning or in a domain-specific manner.\n"," \n","\n","For this reason, state-of-the-art NLP systems usually work best if they are adapted to a specific domain. It is therefore highly desirable to have technology that allows one to adapt an NLP system to a specific domain for MWEs, e.g., on the basis of a text corpus. "],"metadata":{"id":"jkKiP9RVbTvi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vE4ORgfXxCN5","executionInfo":{"status":"ok","timestamp":1647971764621,"user_tz":-330,"elapsed":1126,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"e151746f-f117-44bf-ac28-5da7f9be3054"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n","['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n","['Thanks', '.']\n"]}],"source":["import nltk\n","from nltk.tokenize import MWETokenizer\n","from nltk import sent_tokenize, word_tokenize\n","nltk.download('punkt')\n","s = '''Good cake cost Rs.1500\\kg in Mumbai. Please buy me one of them.\\n\\nThanks.'''\n","mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n","for sent in sent_tokenize(s):\n","  print(mwe.tokenize(word_tokenize(sent)))"]},{"cell_type":"markdown","source":["**B)Aim-:Normalized Web Distance and Word Similarity**"],"metadata":{"id":"hshZkE0yxizC"}},{"cell_type":"code","source":["pip install textdistance"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dl00aRuAyXf2","executionInfo":{"status":"ok","timestamp":1647972029859,"user_tz":-330,"elapsed":4929,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"2108d277-672b-4e13-e36d-03147cb7b32d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textdistance\n","  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n","Installing collected packages: textdistance\n","Successfully installed textdistance-4.2.2\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import re\n","import textdistance \n","# pip install textdistance\n","# we will need scikit-learn>=0.21\n","#pip install sklearn\n","import sklearn \n","from sklearn.cluster import AgglomerativeClustering\n","texts = [\n"," 'Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market',\n"," 'Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport',\n"," 'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading']\n","def normalize(text):\n","  \"\"\" Keep only lower-cased text and numbers\"\"\"\n","  return re.sub('[^a-z0-9]+', ' ', text.lower())\n","def group_texts(texts, threshold=0.4):\n","  \"\"\" Replace each text with the representative of its cluster\"\"\"\n","  normalized_texts = np.array([normalize(text) for text in texts])\n","  distances = 1 - np.array([[textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts ])\n","  clustering = AgglomerativeClustering(distance_threshold=threshold, affinity=\"precomputed\", linkage=\"complete\", n_clusters=None).fit(distances)\n","  centers = dict()\n","  for cluster_id in set(clustering.labels_):\n","    index = clustering.labels_ == cluster_id\n","    centrality = distances[:, index][index].sum(axis=1)\n","    centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n","  return [centers[i] for i in clustering.labels_]\n","print(group_texts(texts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlTxfXbJxNpc","executionInfo":{"status":"ok","timestamp":1647972035914,"user_tz":-330,"elapsed":413,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"2c300900-38ed-41d1-e8f4-f131b87a9d94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"]}]},{"cell_type":"markdown","source":["**C)Aim-:Word Sense Disambiguation**"],"metadata":{"id":"Hb-nBJ7kygqF"}},{"cell_type":"markdown","source":["**Word sense disambiguation**, in natural language processing (NLP), may be defined as the ability to determine which meaning of word is activated by the use of word in a particular context. Lexical ambiguity, syntactic or semantic, is one of the very first problem that any NLP system faces. \n","\n","Part-of-speech (POS) taggers with high level of accuracy can solve Wordâ€™s syntactic ambiguity. On the other hand, the problem of resolving semantic ambiguity is called WSD (word sense disambiguation). Resolving semantic ambiguity is harder than resolving syntactic ambiguity.\n","\n","\n","\n"],"metadata":{"id":"rMx64ddWcXAq"}},{"cell_type":"code","source":["#Word Sense Disambiguation\n","from nltk.corpus import wordnet as wn\n","nltk.download('wordnet')\n","def get_first_sense(word, pos=None):\n","  if pos:\n","    synsets = wn.synsets(word,pos)\n","  else:\n","    synsets = wn.synsets(word)\n","  return synsets[0]\n","best_synset = get_first_sense('bank')\n","print ('%s: %s' % (best_synset.name, best_synset.definition))\n","best_synset = get_first_sense('set','n')\n","print ('%s: %s' % (best_synset.name, best_synset.definition))\n","best_synset = get_first_sense('set','v')\n","print ('%s: %s' % (best_synset.name, best_synset.definition))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQOawKktyStL","executionInfo":{"status":"ok","timestamp":1647972098637,"user_tz":-330,"elapsed":3009,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"20dd8145-6acf-4dbe-88c3-a6d6ac8f5d9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n","<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n","<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"NGVovssmyloz"},"execution_count":null,"outputs":[]}]}
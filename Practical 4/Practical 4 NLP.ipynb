{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practical 4 NLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNgAwe1+HZmkES8LinIGh4p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","# **Practical 4** \n","**Text Tokenization**"],"metadata":{"id":"GQp3r0BA8T78"}},{"cell_type":"markdown","source":["#4A ) Aim-: Tokenization using Python’s split() function"],"metadata":{"id":"pKJGZ8ArPvod"}},{"cell_type":"markdown","source":["**Tokenization**\n","\n","Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n","\n","For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’\n","\n","There are different methods and libraries available to perform tokenization. NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.\n","\n","**Tokenization using Python’s split() function**\n","\n","It returns a list of strings after breaking the given string by the specified separator. By default, split() breaks a string at each space. We can change the separator to anything. "],"metadata":{"id":"4mwNpan78zfH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4Yxk1S5PpLP","executionInfo":{"status":"ok","timestamp":1647962994089,"user_tz":-330,"elapsed":19,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"926a98d3-87ef-477a-9c1a-2cec81655ba2"},"outputs":[{"output_type":"stream","name":"stdout","text":[" This tool is an a beta stage\n"," Alexa developers can use Get Metrics API to\n","seamlessly analyse metric\n"," It also supports custom skill model, prebuilt Flash Briefing\n","model, and the Smart Home Skill API\n"," You can use this tool for creation of monitors,\n","alarms, and dashboards that spotlight changes\n"," The release of these three tools will\n","enable developers to create visual rich skills for Alexa devices with screens\n"," Amazon\n","describes these tools as the collection of tech and tools for creating visually rich and\n","interactive voice experiences\n"," \n"]}],"source":["text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to\n","seamlessly analyse metric. It also supports custom skill model, prebuilt Flash Briefing\n","model, and the Smart Home Skill API. You can use this tool for creation of monitors,\n","alarms, and dashboards that spotlight changes. The release of these three tools will\n","enable developers to create visual rich skills for Alexa devices with screens. Amazon\n","describes these tools as the collection of tech and tools for creating visually rich and\n","interactive voice experiences. \"\"\"\n","data = text.split('.')\n","for i in data:\n","  print (i)"]},{"cell_type":"markdown","source":["#B) Aim-:Tokenization using Regular Expressions (RegEx)\n"],"metadata":{"id":"riGYIN_RQC0j"}},{"cell_type":"code","source":["import nltk\n","# import RegexpTokenizer() method from nltk\n","from nltk.tokenize import RegexpTokenizer\n","\n","# Create a reference variable for Class RegexpTokenizer\n","tk = RegexpTokenizer('\\s+', gaps = True)\n","\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","\n","# Use tokenize method\n","tokens = tk.tokenize(str)\n","\n","print(tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmBkxH_dP4cI","executionInfo":{"status":"ok","timestamp":1647963038203,"user_tz":-330,"elapsed":500,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"37a4c9fa-e198-4e0a-941b-0a286a0f59af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"]}]},{"cell_type":"markdown","source":["#C) Aim-:Tokenization using NLTK"],"metadata":{"id":"81flduRjQJYu"}},{"cell_type":"markdown","source":["**What is NLTK?**\n","\n","NLTK stands for Natural Language Tool Kit, which is considered to be the most powerful NLP libraries. NLP (Natural Language Processing) is a technique that helps in manipulation and working with text or speech with the help of software and devices. It draws patterns based on the context in which statements are being presented. "],"metadata":{"id":"S6NRT_tZ-LWQ"}},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUELfZyGQQfJ","executionInfo":{"status":"ok","timestamp":1647963085994,"user_tz":-330,"elapsed":2623,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"b0f04dfb-b5ef-4f55-9ceb-22f572868875"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","\n","# Use tokenize method\n","print(word_tokenize(str))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2izpAZbQFl4","executionInfo":{"status":"ok","timestamp":1647963087999,"user_tz":-330,"elapsed":11,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"08e14d06-eb4d-47ae-ca05-8cef3f3bc058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"]}]},{"cell_type":"markdown","source":["#D) Aim-:Tokenization using the spaCy library"],"metadata":{"id":"fUIWEA7MQWEL"}},{"cell_type":"code","source":["import spacy\n","nlp = spacy.blank(\"en\")\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","# Create an instance of document;\n","# doc object is a container for a sequence of Token objects.\n","doc = nlp(str)\n","# Read the words; Print the words\n","#\n","words = [word.text for word in doc]\n","print(words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJunt6rHQKmI","executionInfo":{"status":"ok","timestamp":1647963113886,"user_tz":-330,"elapsed":1455,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"57ea6168-2295-4e70-dd5f-b40112df5dd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"]}]},{"cell_type":"markdown","source":["#E )Aim-:Tokenization using Keras"],"metadata":{"id":"MewhVrk5Qk1J"}},{"cell_type":"markdown","source":["**Keras Tokenizer Class**\n","\n","The Tokenizer class of Keras is used for vectorizing a text corpus. For this either, each text input is converted into integer sequence or a vector that has a coefficient for each token in the form of binary values."],"metadata":{"id":"QqQEwEPd-t5Q"}},{"cell_type":"code","source":["pip install keras\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eoc7uIo3QYAa","executionInfo":{"status":"ok","timestamp":1647963136566,"user_tz":-330,"elapsed":3511,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"77473eda-733f-4853-d107-d728252acd1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"]}]},{"cell_type":"code","source":["pip install tensorflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbrCcK41Qc0e","executionInfo":{"status":"ok","timestamp":1647963143066,"user_tz":-330,"elapsed":3677,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"13406af3-0bf4-4a10-ff4a-1b4450775921"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 13.3 MB/s \n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n","Installing collected packages: tf-estimator-nightly\n","Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"]}]},{"cell_type":"code","source":["import keras\n","from keras.preprocessing.text import text_to_word_sequence\n","str = \"I love to study Natural Language Processing in Python\"\n","# tokenizing the text\n","tokens = text_to_word_sequence(str)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0zSDtSdQejf","executionInfo":{"status":"ok","timestamp":1647963156638,"user_tz":-330,"elapsed":3059,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"e7dac606-1585-4d99-c6d8-a3c1edd2a646"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"]}]},{"cell_type":"markdown","source":["#F) Aim-:Tokenization using Gensim"],"metadata":{"id":"f57cjvraQrTK"}},{"cell_type":"markdown","source":["\n","**Tokenization using Gensim**\n","\n","The final tokenization method we will cover here is using the Gensim library. It is an open-source library for unsupervised topic modeling and natural language processing and is designed to automatically extract semantic topics from a given document.\n","\n","We can use the gensim.utils class to import the tokenize method for performing word tokenization."],"metadata":{"id":"sT6UMz3-_XiJ"}},{"cell_type":"code","source":["pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WYh6k-wQh8-","executionInfo":{"status":"ok","timestamp":1647963202299,"user_tz":-330,"elapsed":2109,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"3bab37be-c51c-44eb-d4bd-c9b78edc56c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n"]}]},{"cell_type":"code","source":["\n","from gensim.utils import tokenize\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","# tokenizing the text\n","list(tokenize(str))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-Cer97uQtIG","executionInfo":{"status":"ok","timestamp":1647963203051,"user_tz":-330,"elapsed":7,"user":{"displayName":"19_neha_dawale","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16793797596584424738"}},"outputId":"487e060c-d09e-42d6-f87f-f1899c6aa5a6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I',\n"," 'love',\n"," 'to',\n"," 'study',\n"," 'Natural',\n"," 'Language',\n"," 'Processing',\n"," 'in',\n"," 'Python']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[""],"metadata":{"id":"UQ-e0JHYQt-6"},"execution_count":null,"outputs":[]}]}